# -*- coding: utf-8 -*-
"""Hands_on_Data_Visualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1auKDkSK-wdE9kB5s14NkvvA_nbc1JPP3

# Hands-on 6: Data-Engineering II - Data Visualization

Note: This lab session is graded. Complete all the exercises, run your code and upload the ipynb file under assignment **Hands-on: Data Visualization**

**Deadline is Sunday (10/01/), 11:59 PM**. However it is strongly recommended that you complete the exercises in the class.

## Data Collection for Weather Analysis

Last week, we scraped the EstesPark website and collected data for the month of August 2023.
https://www.estesparkweather.net/archive_reports.php?date=202308.

Today, we will try to perform data analysis where we extract descriptive statistics and apply data visualization techniques to gain some insights

We would need the following Python libraries :

**Pandas:** This is the library for dealing with Tablular data. Also allows some statistical opetations.

**NumPy/Scipy:** Library for statistical analysis

**Matplotlib:** For plotting data

## 0. Install and Import Libraries
"""

## Import libraries
## if you see any import error, install libraries through pip install command
import pandas as pd
import numpy as np
import scipy
from matplotlib import pyplot as plt
print ("Imported all libraries successfully...")

"""## 1. Load the EstesPark data we collected

If you did the last hands-on, you should have the data on your local folder by the name **EstesPark_Weather_August_2023.csv**. If you do not have it, you could download it from **Canvas->Files->Week6->EstesPark_Weather_August_2023.csv**

Try to open the file in Excel / Numbers / Open office (just doube click on it to open with your default app for numbers).
"""

# Before starting, let's tell jupyter not to ignore any output that is supposed to be printed
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

estes_park_df = pd.read_csv("EstesPark_Weather_August_2023.csv")

estes_park_df.info()

"""Print a few rows."""

estes_park_df.head()

"""## 1.1. What kind of variables do we have?

Check the output from the info method above. There are two kinds of columns (variables)

- Numerical (int, float): Average temperature, Average humidity, Average dewpoint, ...
- Categorical (strings): Date

## 2. Print column names and access column data?
Say you want the average_temperature column as a list type object
"""

column_names = estes_park_df.columns
print (column_names)

average_temperature = estes_park_df["Average temperature"]
print (average_temperature)

"""## 3. Descriptive Stats using df.describe() method"""

average_temperature.describe()

"""**Insights:**
The range of the temperature is: [-0.8, 42.9]. Also, the mean temperature is below freezing point (~25F) on an average with a standard deviation of 10.21F. Estes Park was  generally cold during the month of January 2023.

### 3.1. Frequency plot using df.hist() method

Pandas DataFrames already provide a plotting function that can bin our data into frequency buckets and then plot frequencies. Frequency plots are called **histograms**.
"""

average_temperature.hist(bins=10)

"""#### 3.1.1 What does the histogram represent?

On x-axis, we have the values of temperature binned into 10 bins. Based on the range i.e., [-0.8, 42.9] and the bin size of 10, the bins are:

- bin1: [-0.8,3.57]
- bin2: [3.58,7.94]
- ...
- bin10: [38.53,42.9]

The y-axis represents number of times temperature values fall under a certain bin (i.e., frequency). E.g., *bin1* has 2 temperature values. *bin9* has 4 values.

Let's try and keep the binsize same as the number of rows
"""

average_temperature.hist(bins=len(average_temperature))

"""Try some other values?"""

average_temperature.hist(bins=5)

"""Looks somewhat like a bell curve? Let's revisit the empirical rules we came across in the lecture.

If you plot your data based on frequency and it looks "normal"
- about 68% of data within 1 standard dev. of mean
- about 95% of data within 2 standard dev. of mean
- all or nearly all data within 3 standard dev. of mean

**Insights:** In our case `mean + 2*stdev ~ 44F` which represents the fact that ~68% of the time we will have temperature less than 44F. So we can confidently say that on any day in the month of January 2023, if we planned a visit to Estes Park, we would have had to do a lot of winter prep.

## E1. Exercise: Describe and Plot Histogram for "Average humidity"
"""

#find all the data points for humitiy (average)
average_humidity=estes_park_df["Average humidity"]
#descibe various statistics of the collect of Average humdidty (mean, stadnard deviation, etc.)
average_humidity.describe()
#find how many of each average humitidity there is
#plot on frequency chart
average_humidity.hist(bins=8)

"""What Can you try different bin sizes for the histogram? What insights do you get?

## 3.2. Bivariate Analysis
We are going to study the association between two columns (i.e., two variables). For this we compute the Pearson correlction coefficient between two variables.

Let's say our two variables of choice are (a) Average windspeed and (b) Average gustspeed.
"""

from scipy.stats import pearsonr

p = pearsonr(estes_park_df["Average temperature"],estes_park_df["Average humidity"])
print (p[0])

"""**Insight:** The first value above represents the corelation coefficient, whcih is ~**0.998**. This means windspeed is strongly correlated with gust speed, which is expected.

Now let's try to extract a correlation matrix for all the **numerical columns**

**Insights:**
- Temperature and humidity are strongly "negatively" correlated at -0.578 which indicates that as the temperature (especially in the cold weather in January 2023) rises, it will probably result in a less humid atmosphere.
- Temperature and pressure are strongly "negatively" correlated at -0.482231 which indicates that a rise in temperature will reduce atmospheric pressure, which is expected.
- Humidity and dew points are very weakly correlated at 0.101796 and it appears that there is no relationship between them. (Is this expected?)

Any other insights?

## 4. Data visualization

## 4.1. Bar Graphs
A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent.

In the above data, the only categorical data is "Date" (we are of-course ignoring the  unprocessed columns from Average direction onwards).

Say we want to plot Average Temperature for **five random dates**, without maintaining sequentiality. We can go for a bar graph.
"""

five_random_days = [1,4,8,19,25]

date_column = estes_park_df["Average and Extremes"]
temp_column = estes_park_df["Average temperature"]

selected_dates = []
selected_temps = []

for day_id in five_random_days:
    selected_dates.append(date_column[day_id])
    selected_temps.append(temp_column[day_id])


# Now plot our bar graph
plt.bar(selected_dates, selected_temps, color ='maroon', width = 0.4)

# Now add axis labels with units
plt.xlabel("Date")
plt.ylabel("Average per-day temperature in 'F")

# We can add a title too
plt.title("Temperature for 5 random days August 2023")

"""## 4.2. Line Graphs

- Best used when the x-axis represents a sequence (e.g., time) and y-axis is a continuous numerical variable.

We can approximate days in January 2023 as a representation of sequentiality (time) and plot temperature-vs-day as a line graph
"""

day_ids = range(len(date_column)) # creates a variable 0,1,2...total_days
temp_column = estes_park_df["Average temperature"]

plt.plot(day_ids,temp_column, marker='o')

plt.xlabel("Day number")
plt.ylabel("Average per-day temperature in 'F")

# We can add a title too
plt.title("Temperature progression in January 2023")

"""## 4.3. Scatter Plots

- Best for representing relationship between two continuous variables
- Can add third continuous variable represented by marker size
- Can add categorical information by changing marker color

Let's plot windspeed and gust speed together.
"""

windspeed_column = estes_park_df["Average windspeed"]
gustspeed_column = estes_park_df["Average gustspeed"]

plt.scatter(windspeed_column,gustspeed_column, marker="*")

plt.xlabel("Average windspeed (in mph)")
plt.ylabel("Average gustspeed (in mph)")

# We can add a title too
plt.title("gustspeed-vs-windspeed in January 2023")

"""Does this corroborate with our correlation analysis in 3.2?

Now let's draw a scatter plot considering three variables where the third variable is represented by marker size. The variables are:

- Average temperature
- Average pressure (barometer)
- Average humidity
"""

temp_column = estes_park_df["Average temperature"]
pressure_column = estes_park_df["Average barometer"]
humidity_column = estes_park_df["Average humidity"]

plt.scatter(temp_column, pressure_column, marker="o", s=humidity_column)

plt.title("Average barometer vs Average temperature (size of circle = average humidity)")

plt.xlabel("Average temperature (in 'F)")
plt.ylabel("Average pressure (in mmhg)")

"""## 4.4. Box Plots
- A Box Plot is also known as Whisker plot is created to display the summary of the set of data values for a variable
- Properties like minimum, first quartile, median, third quartile and maximum.
- Also plots outliers

Let's plot Average temperature using boxplot.
"""

temp_column = estes_park_df["Average temperature"]

plt.boxplot(temp_column)

plt.xlabel("Temperature")
plt.ylabel("Temp in 'F")

"""## E2. Exercise: IMDB Movie Data

- Download the IMDB Movie Data (extracted from Kaggle's IMDB Movie Dataset posting) from **Canvas->Files->Week6->IMDB_movie_metadata_small.csv**
- Load the data to a Pandas dataframe using read_csv() method
- Which columns are categorical and which ones are numerical? Write in a markdown block.
- Describe columns "gross" (represents total revenue made by the movie) and "imdb_score" using describe(). Write your insights in a markdown block.
- Analyze "imdb_score" and "gross" together by computing the Pearson's correlation coefficient. What does it tell you? Write down in a markdown block.
- Plot "gross" vs "director_name" for the **first 10 rows**. Which form of plot are you going to use?
- Plot "imdb_score"-vs-"movie_facebook_likes". Which form of plot are you going to use?
- Boxplot "gross" and write down your insights from the box plot.

#all coloums will be categorical variables (Movie identity, director name, imdb score, movie facebook likes, gross, year)
#the following rows will be catergorical: directory and movie ID
#all other rows will be numerical
"""

#python libraries
import pandas as pd
import numpy as np
import scipy
from matplotlib import pyplot as plt
from scipy.stats import pearsonr
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
#create a table for the IMDB moview dataset
IMDB_movie_metadata=pd.read_csv("IMDB_movie_metadata_small.csv")
#all coloums will be categorical variables (Movie identity, director name, imdb score, movie facebook likes, gross, year)
column_names = IMDB_movie_metadata.columns
Movie_ID = IMDB_movie_metadata["Movie_ID"]
Directors = IMDB_movie_metadata["director_name"]
Facebook_likes=IMDB_movie_metadata["movie_facebook_likes"]
imdb_score = IMDB_movie_metadata["imdb_score"]
gross = IMDB_movie_metadata["gross"]
year = IMDB_movie_metadata["title_year"]
#the following rows will be catergorical: directory and movie ID
#all other rows will be numerical
#finds stats on gross coloum (mean, std, etc.)
gross.describe()
imdb_score.describe()
#find stats on imdb score coloum (mean, std, etc.)
#look for correlation between imdb score and gross
p = pearsonr(imdb_score,gross)
print(p)
#create a scatter plot of the data points (imdb score, gross)
plt.scatter(imdb_score,gross,marker='o')
plt.xlabel("Movie IMDB Score")
plt.ylabel("Gross Income of Movie ($100,000,000)")
plt.title("Moive IMDB Scores vs Gross Income of Movies ($)")

"""#Gross
There is a big difference between mean and median (mean>median) meaning there is a right skew.
There is also a large STD
This means that there is not a lot of consistency in the gross incomes of the list movies.

#IMDB Score
The IMDB socres follow are fairly normal distrubtion (mean~=median). IMDB scores are fairly consistent, with most being above a rating of 5 with some low outliers.

#Correlation: IMDB Score vs Gross
There is little correlation between the IMDB score and gross income (~0.25)

"""

#create bar graph of directors (x) and gross (y)
plt.bar(Directors,gross)
plt.xlabel("Directors (name)")
plt.ylabel("Gross Income of Movie ($)")
plt.title("Gross Income of Movies by Different Directors")

#create a scatter plot of the data points (imdb score, movie facebook likes)
plt.scatter(imdb_score, Facebook_likes, marker="o")
plt.xlabel("Movie IMDB Score")
plt.ylabel("Number of Facebook Likes")
plt.title("Movie IMDB Score vs Number of Facebook Likes")

#create a boxplot of gross
plt.boxplot(gross)
plt.ylabel("Amount in Dollars ($100,000,000)")
plt.title("Gross Incomes of Movies")

"""Most movies make less than 100,000,000 dollars but those on the higher end can generally make 150 million dollars. There are also a number of bigger movies that make far more than this (up to $700+ million) but these are outliers and are not something you see for most movies.

## [Optional] Pro-tip: Go through Seaborn tutorial

Seaborn is a library mostly used for statistical plotting in Python. It is built on top of Matplotlib and provides beautiful default styles and color palettes to make statistical plots more attractive.

Tutorial here:

https://www.geeksforgeeks.org/python-seaborn-tutorial/
"""